{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Frequent Itemset Mining on health data Implementation\n",
    "\n",
    "Contains three main functions:\n",
    "- apply_fim: apply the method using defined parameters to a sample size population; creates a report with generated clusters in output_files folder\n",
    "- generalize_timeline: use the result of apply_fim to generalize timeline of one person; stores a generalized_timeline values that can be used in health_event_timelines.ipynb notebook\n",
    "- test_fim: test using multiple values and iterations; creates multiple report files with generated clusters in output_files folder\n",
    "\n",
    "Usage:\n",
    "- uncomment functions\n",
    "- set values for parameters\n",
    "- run cells"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68364f029875f8e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpmax\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import random\n",
    "import statistics\n",
    "\n",
    "# Received from the corresponding OMOP format database tables\n",
    "%store -r df_condition_occurrence # condition_occurrence\n",
    "%store -r df_drug_exposure # drug_exposure\n",
    "%store -r df_concept # concept\n",
    "%store -r df_procedure # procedure_occurrence\n",
    "\n",
    "# Received from test_cases.ipynb\n",
    "%store -r use_case_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# FIM Parameters (number of person, minimum threshold support, number of clusters, window length)\n",
    "num_person = 1\n",
    "min_support = 0.001\n",
    "num_cluster = 5\n",
    "win_length = 30\n",
    "\n",
    "# Alternative Method Selection\n",
    "use_alt_sliding_window_method = False\n",
    "use_alt_clustering_method = True\n",
    "\n",
    "# Output File Name\n",
    "output_file = 'output'\n",
    "\n",
    "# fim_result = apply_fim(num_person, min_support, win_length, num_cluster, use_alt_sliding_window_method, use_alt_clustering_method, output_file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7041ac2a10443509"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ID Of Person To Be Generalized\n",
    "person_id = 5\n",
    "\n",
    "generalized_timeline = generalize_timeline(5, fim_result, win_length)\n",
    "%store generalized_timeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3075337d7dff2231"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test FIM Parameters\n",
    "win_lengths = [15, 30]\n",
    "num_clusters = [5, 10]\n",
    "\n",
    "# Number Of Iterations\n",
    "iterations_per_test = 1\n",
    "\n",
    "# test_fim(num_clusters, win_lengths, iterations_per_test, num_person, min_support, use_alt_sliding_window_method, use_alt_clustering_method)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf189d561b50a26c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_datasets(person_nr, person_id):\n",
    "    person_ids = list(set(df_condition_occurrence[\"person_id\"].values.tolist()))\n",
    "    selected_person_ids = random.sample(person_ids, person_nr) if person_id is None else [person_id]\n",
    "    \n",
    "    condition_occurrence_dataset = df_condition_occurrence[df_condition_occurrence['person_id'].isin(selected_person_ids)]\n",
    "    drug_exposure_dataset = df_drug_exposure[df_drug_exposure['person_id'].isin(selected_person_ids)]\n",
    "    procedure_dataset = df_procedure[df_procedure['person_id'].isin(selected_person_ids)]\n",
    "    \n",
    "    drug_exposure_dataset = drug_exposure_dataset.rename(columns={\n",
    "        'person_id': 'person_id',\n",
    "        'drug_concept_id': 'concept_id',\n",
    "        'drug_exposure_start_date': 'start_datetime'\n",
    "    })\n",
    "    \n",
    "    condition_occurrence_dataset = condition_occurrence_dataset.rename(columns={\n",
    "        'person_id': 'person_id',\n",
    "        'condition_concept_id': 'concept_id',\n",
    "        'condition_start_date': 'start_datetime'\n",
    "    })\n",
    "    \n",
    "    procedure_dataset = procedure_dataset.rename(columns={\n",
    "        'person_id': 'person_id',\n",
    "        'procedure_concept_id': 'concept_id',\n",
    "        'procedure_date': 'start_datetime'\n",
    "    })\n",
    "    \n",
    "    medical_data_df = pd.concat([\n",
    "        drug_exposure_dataset[['person_id', 'concept_id', 'start_datetime']],\n",
    "        procedure_dataset[['person_id', 'concept_id', 'start_datetime']],\n",
    "        condition_occurrence_dataset[['person_id', 'concept_id', 'start_datetime']]\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    all_concept_ids = medical_data_df['concept_id'].tolist()\n",
    "    all_concept_ids_counts = medical_data_df['concept_id'].value_counts()\n",
    "    \n",
    "    if person_id is None and len(selected_person_ids) > 100:\n",
    "        medical_data_df = medical_data_df[medical_data_df['concept_id'].map(all_concept_ids_counts) >= 10]\n",
    "    medical_data_df = medical_data_df[medical_data_df['concept_id'] != 0]\n",
    "    \n",
    "    event_mappings = []\n",
    "    \n",
    "    for person_id in selected_person_ids: \n",
    "        person_dataset = medical_data_df[medical_data_df['person_id'] == person_id]\n",
    "        dict_with_codes = format_dataset_to_dict(person_dataset)\n",
    "        dict_with_codes_sorted = {k: dict_with_codes[k] for k in sorted(dict_with_codes.keys())}\n",
    "        event_mappings.append(dict_with_codes_sorted)\n",
    "    \n",
    "    return event_mappings, all_concept_ids, all_concept_ids_counts.to_dict(), medical_data_df['start_datetime'].min()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cf20fb37fbdc6e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def assign_integer_to_datetime(datetime, start_date, end_date, total):\n",
    "    if datetime >= end_date:\n",
    "        return total\n",
    "    else:\n",
    "        return (datetime - start_date).days"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9d04ed5f2a288af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sliding_window_method_1(events_dict, max_days_difference):\n",
    "    result_dict = {}\n",
    "    for key in sorted(events_dict.keys()):\n",
    "        group_key = int(key/max_days_difference)\n",
    "        if group_key in result_dict:\n",
    "            result_dict[group_key].extend(events_dict[key])\n",
    "        else:\n",
    "            result_dict[group_key] = events_dict[key]\n",
    "    return [list(set(value)) for value in result_dict.values()]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65b830815921879"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sliding_window_method_2(events_dict, window_length):\n",
    "    items_list = []\n",
    "    keys = list(events_dict.keys())\n",
    "\n",
    "    for i in range(len(keys)):\n",
    "        window_items = []\n",
    "        key = keys[i]\n",
    "        window_items.extend(events_dict[key])\n",
    "        j = 1\n",
    "        while (i + j) < len(keys):\n",
    "            if keys[i + j] <= (key + window_length):\n",
    "                window_items.extend(events_dict[keys[i + j]])\n",
    "                j += 1\n",
    "            else:\n",
    "                break\n",
    "        items_list.append(list(set(window_items)))\n",
    "\n",
    "    return items_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a9bd3c3a50ffaa1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_itemsets_with_time(events_dict, window_length):\n",
    "    result_dict = {}\n",
    "    for key in sorted(events_dict.keys()):\n",
    "        group_key = int(key/window_length)\n",
    "        if group_key in result_dict:\n",
    "            result_dict[group_key].extend(events_dict[key])\n",
    "        else:\n",
    "            result_dict[group_key] = events_dict[key]\n",
    "    return result_dict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1dce7b481396fda"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def format_dataset_to_dict(dataset):\n",
    "    min_start_date = dataset['start_datetime'].min()\n",
    "    max_start_date = dataset['start_datetime'].max()\n",
    "    difference = max_start_date - min_start_date\n",
    "    \n",
    "    result_dict = {}\n",
    "    for item in dataset.values:\n",
    "        number = assign_integer_to_datetime(item[2], min_start_date, max_start_date, difference.days)\n",
    "        value = str(item[1])\n",
    "        if number not in result_dict:\n",
    "            result_dict[number] = []\n",
    "        if value not in result_dict[number]:\n",
    "            result_dict[number].append(value)\n",
    "\n",
    "    return result_dict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c7df3692cb2cb8c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_item_sets(event_dict_list, window_length, sliding_window_method):\n",
    "    result_list = []\n",
    "    for event_dict in event_dict_list:\n",
    "        if not sliding_window_method:\n",
    "            result_list.extend(sliding_window_method_1(event_dict, window_length))\n",
    "        else:\n",
    "            result_list.extend(sliding_window_method_2(event_dict, window_length))\n",
    "    return result_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "661f4cb1e8968c8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_tf_idf_all_clusters(clusters):\n",
    "    list_of_cluster_dicts = []\n",
    "    for i in range(len(clusters)):\n",
    "        cluster_tf_idf_result = calculate_tf_idf(clusters[i], clusters, len(clusters))\n",
    "        list_of_cluster_dicts.append(dict(cluster_tf_idf_result))\n",
    "    return list_of_cluster_dicts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "952eefd13a1056d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def merge_dicts_with_highest_score(list_of_dicts):\n",
    "    merged_dict = {}\n",
    "    for d in list_of_dicts:\n",
    "        key = list(d.keys())[0]\n",
    "        if key in merged_dict:\n",
    "            merged_dict[key].extend(list(d.keys()))\n",
    "            merged_dict[key] = list(set(merged_dict[key]))\n",
    "        else:\n",
    "            merged_dict[key] = list(d.keys())\n",
    "            \n",
    "    modified_dict = {}\n",
    "\n",
    "    for key, values in merged_dict.items():\n",
    "        if key in values:  \n",
    "            values.remove(key) \n",
    "            values.insert(0, key)\n",
    "        modified_dict[key] = values\n",
    "    return modified_dict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "848fb239a7f0b388"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_tf_idf(cluster_items, concat_clusters, n_clusters):\n",
    "    cluster_features = list(set(cluster_items))\n",
    "    result_tf_scores = []\n",
    "    \n",
    "    for feature in cluster_features:\n",
    "        tf = cluster_items.count(feature)/len(cluster_items)\n",
    "        feature_count = sum(1 for cluster_items in concat_clusters if feature in cluster_items)\n",
    "        idf = math.log(n_clusters/feature_count, 10)\n",
    "        tf_idf = round(tf * idf, 3)\n",
    "        result_tf_scores.append((feature, tf_idf))\n",
    "    sorted_result_by_tf_idf_score = sorted(result_tf_scores, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_result_by_tf_idf_score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b1e4a21b6b992c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_medical_label_by_concept_id(concept_id):\n",
    "    concept_id = int(concept_id)\n",
    "    concept_labels = df_concept.loc[df_concept['concept_id'] == concept_id, 'concept_name'].values\n",
    "    if len(concept_labels) > 0:\n",
    "        return concept_labels[0]\n",
    "    else:\n",
    "        return \"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7678dadc986ce96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_clusters_per_use_case(cluster_concept_ids, use_cases):\n",
    "    counts = []\n",
    "    for use_case in use_cases:\n",
    "        count = 0\n",
    "        for cluster in cluster_concept_ids:\n",
    "            if set(use_case) & set(cluster):\n",
    "                count += 1\n",
    "        counts.append(count)\n",
    "    return round(sum(counts) / len(use_cases), 5), round(statistics.stdev(counts), 5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "404aa75c33d0a484"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_use_cases_per_cluster(cluster_concept_ids, use_cases):\n",
    "    counts = []\n",
    "    for cluster in cluster_concept_ids:\n",
    "        count = 0\n",
    "        for use_case in use_cases:\n",
    "            if set(use_case) & set(cluster):\n",
    "                count += 1\n",
    "        counts.append(count)\n",
    "    filtered_counts = [item for item in counts if item != 0]\n",
    "    if len(filtered_counts) < 1:\n",
    "        return 0, 0\n",
    "    return round(sum(filtered_counts) / len(filtered_counts), 5), round(statistics.stdev(filtered_counts), 5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73195e49a0bafec8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_score_of_use_cases_in_item_sets(cluster_concept_ids):\n",
    "    cluster_concept_ids_set = set(sum(cluster_concept_ids, []))\n",
    "    use_case_list_set = set(sum(use_case_list, []))\n",
    "    return round(len(use_case_list_set.intersection(cluster_concept_ids_set)) / len(use_case_list_set), 5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73ac18de2b9c272b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_frequency_score_of_missing_concept_ids(cluster_concept_ids, all_concept_ids, all_concept_ids_counts):\n",
    "    concept_id_count = len(all_concept_ids)\n",
    "    cluster_concept_ids_set = set(sum(cluster_concept_ids, []))\n",
    "    frequent_items_count = 0\n",
    "    for cluster_concept_id in cluster_concept_ids_set:\n",
    "        frequent_items_count += all_concept_ids_counts[int(cluster_concept_id)]\n",
    "    return round((frequent_items_count / concept_id_count), 5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36b9e14ec78a5fb5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_clusters_alternative(frequent_item_sets_list):\n",
    "    tf_idf_result = calculate_tf_idf_all_clusters(frequent_item_sets_list)\n",
    "    merged = merge_dicts_with_highest_score(tf_idf_result)\n",
    "    return list(merged.values())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2413cc20c359c516"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def apply_fim_to_dataset(support_threshold, n_cluster, window_length, event_mappings, all_concept_ids, all_concept_ids_counts, output_file, sliding_window_method, use_alt_clustering_method):\n",
    "    item_sets = generate_item_sets(event_mappings, window_length, sliding_window_method)\n",
    "    print(f\"{output_file} - item sets created\")\n",
    "    frequent_item_sets_list = generate_frequent_item_sets(item_sets, support_threshold)\n",
    "    print(f\"{output_file} - frequent item sets created\")\n",
    "    if use_alt_clustering_method:\n",
    "        print(f\"{output_file} - clusters created\")\n",
    "        return validate_and_score_clusters(create_clusters_alternative(frequent_item_sets_list), all_concept_ids_counts, all_concept_ids, output_file, window_length, support_threshold, True)\n",
    "    else:\n",
    "        clusters = create_clusters(n_cluster, frequent_item_sets_list)\n",
    "        print(f\"{output_file} - clusters created\")\n",
    "        return validate_and_score_clusters(clusters, all_concept_ids_counts, all_concept_ids, output_file, window_length, support_threshold, use_alt_clustering_method)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "299fc7d35f7931a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_frequent_item_sets(item_sets, support_threshold):\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(item_sets).transform(item_sets)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    \n",
    "    frequent_itemsets = fpmax(df, min_support=support_threshold, use_colnames=True)\n",
    "    frequent_itemsets_list = []\n",
    "    \n",
    "    for index, row in frequent_itemsets.iterrows():\n",
    "        items = list(row['itemsets'])\n",
    "        frequent_itemsets_list.append(items)\n",
    "    \n",
    "    return [inner_list for inner_list in frequent_itemsets_list if 1 < len(inner_list)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17f60dbfc8847880"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_clusters(n_cluster, frequent_item_sets):\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    frequent_item_sets_string_list = [' '.join(sublist) for sublist in frequent_item_sets]\n",
    "    bow = vectorizer.fit_transform(frequent_item_sets_string_list).toarray()\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_cluster, random_state=42, n_init='auto')\n",
    "    clusters = kmeans.fit_predict(bow)\n",
    "\n",
    "    cluster_data = {}\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        if cluster not in cluster_data:\n",
    "            cluster_data[cluster] = []\n",
    "        cluster_data[cluster].append(frequent_item_sets[i])\n",
    "       \n",
    "    cluster_data_sorted = sorted(cluster_data.items())\n",
    "    \n",
    "    cluster_data = {}\n",
    "    concat_clusters = []\n",
    "    for key, value in cluster_data_sorted:\n",
    "        cluster_data[key] = value\n",
    "    \n",
    "    for cluster, data_points in cluster_data.items():\n",
    "        concat_clusters.append(sum(data_points, []))\n",
    "    \n",
    "    return concat_clusters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7b289b453b3227b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def validate_and_score_clusters(clusters, all_concept_ids_counts, all_concept_ids, output_file, window_length, support,\n",
    "                                use_alt_clustering_method):\n",
    "    with open(\"../output_files/\" + output_file + \".txt\", \"w\") as f:\n",
    "        print(output_file + \".txt\", file=f)\n",
    "        print(\"---------------------------\", file=f)\n",
    "        print(f\"Number of clusters: {len(clusters)}\", file=f)\n",
    "        print(f\"Window length: {window_length}\", file=f)\n",
    "        print(f\"Minimum support: {support}\", file=f)\n",
    "        print(f\"Number of use cases: {len(use_case_list)}\", file=f)\n",
    "        print(\"---------------------------\", file=f)\n",
    "        clusters_per_use_case = calculate_clusters_per_use_case(clusters, use_case_list)\n",
    "        use_cases_per_cluster = calculate_use_cases_per_cluster(clusters, use_case_list)\n",
    "        print(f\"1) Average number of clusters per use case: {clusters_per_use_case[0]}\", file=f)\n",
    "        print(f\"2) SD of clusters per use case: {clusters_per_use_case[1]}\", file=f)\n",
    "        print(f\"3) Average number of use cases per cluster: {use_cases_per_cluster[0]}\", file=f)\n",
    "        print(f\"4) SD of use cases per cluster: {use_cases_per_cluster[1]}\", file=f)\n",
    "        print(f\"5) Use case concept ids in frequent item sets: {calculate_score_of_use_cases_in_item_sets(clusters)}\", file=f)\n",
    "        print(\n",
    "            f\"6) Concept ids included in clusters with frequency: {calculate_frequency_score_of_missing_concept_ids(clusters, all_concept_ids, all_concept_ids_counts)}\",file=f)\n",
    "        print(\"---------------------------\", file=f)\n",
    "        print(\" \", file=f)\n",
    "\n",
    "        list_of_cluster_dicts = []\n",
    "        if use_alt_clustering_method:\n",
    "            for i in range(len(clusters)):\n",
    "                cluster_tf_idf_result = calculate_tf_idf(clusters[i], clusters, len(clusters))\n",
    "                list_of_cluster_dicts.append(dict(cluster_tf_idf_result))\n",
    "                print(f\"Cluster {i}:\", file=f)\n",
    "                for concept in clusters[i]:\n",
    "                    print(f\"{concept}; {get_medical_label_by_concept_id(concept)}\", file=f)\n",
    "                print(\" \", file=f)\n",
    "        else:\n",
    "            for i in range(len(clusters)):\n",
    "                cluster_tf_idf_result = calculate_tf_idf(clusters[i], clusters, len(clusters))\n",
    "                list_of_cluster_dicts.append(dict(cluster_tf_idf_result))\n",
    "                print(f\"Cluster {i}:\", file=f)\n",
    "                for count, tf_idf_result in enumerate(cluster_tf_idf_result):\n",
    "                    print(f\"{tf_idf_result[0]}; {tf_idf_result[1]} {get_medical_label_by_concept_id(tf_idf_result[0])}\", file=f)\n",
    "                print(\" \", file=f)\n",
    "        return list_of_cluster_dicts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e1699f01dcb1e58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def assign_cluster_to_event_method_1(event, clusters):\n",
    "    cluster_index = -1\n",
    "    tf_idf = 0\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        if event in cluster:\n",
    "            if cluster[event] > tf_idf:\n",
    "                cluster_index = idx\n",
    "                tf_idf = cluster[event]\n",
    "    return cluster_index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb918542391dc950"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def convert_months_to_date(start_date, months, window_length):\n",
    "    return (start_date.to_pydatetime() + timedelta(days=months * window_length)).strftime(\"%Y-%m-%d\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da3cb912e6b6c38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def assign_clusters_to_events(events, clusters):\n",
    "    return list({assign_cluster_to_event_method_1(event, clusters) for event in events})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e94446da5e8242d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def assign_itemsets_to_clusters(itemsets, clusters, start_date, window_length):\n",
    "    result_dict = {key: assign_clusters_to_events(value, clusters) for key, value in itemsets.items()}\n",
    "    result_list = [key for key, value in result_dict.items() for _ in range(len(value))]\n",
    "    cluster_indexes = [item for sublist in result_dict.values() for item in sublist]\n",
    "    return [list(map(lambda months: convert_months_to_date(start_date, months, window_length), result_list)), cluster_indexes]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "463450b56347afaf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generalize_timeline(person_id, clusters, window_length):\n",
    "    data = create_datasets(None, person_id)\n",
    "    itemsets = generate_itemsets_with_time(data[0][0], window_length)\n",
    "    generalized_data = assign_itemsets_to_clusters(itemsets, clusters, data[3], window_length)\n",
    "    cluster_labels = create_labels_for_clusters(clusters)\n",
    "    cluster_ids = generalized_data[1]\n",
    "    labelled_clusters = [\"Nan\" if cluster_id == -1 else cluster_labels[cluster_id][:30] for cluster_id in cluster_ids]\n",
    "    generalized_data[1] = labelled_clusters\n",
    "        \n",
    "    return generalized_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30f48974bf05f71b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_labels_for_clusters(clusters):\n",
    "    top_concept_ids = [next(iter(d)) for d in clusters]\n",
    "    return list(map(get_medical_label_by_concept_id, top_concept_ids))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1ee5caa383189b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_fim(num_of_clusters, window_lengths, iterations, n_person, support, use_alt_sliding_window_method, use_alt_clustering_method):\n",
    "    for i in range(iterations):\n",
    "        event_mappings, all_concept_ids, all_concept_ids_counts, _ = create_datasets(person_nr=n_person, person_id=None)\n",
    "        print(f\"{i} - dataset created\")\n",
    "        test_index = 1\n",
    "        for j in range(len(window_lengths)):\n",
    "            item_sets = generate_item_sets(event_mappings, window_lengths[j], use_alt_sliding_window_method)\n",
    "            item_set_idx = str(i) + \"_\" + str(j)\n",
    "            print(f\"{item_set_idx} - item sets created\")\n",
    "            frequent_item_sets_list = generate_frequent_item_sets(item_sets, support)\n",
    "            print(f\"{item_set_idx} - frequent item sets created\")\n",
    "            for k in range(len(num_of_clusters)):\n",
    "                output_file = str(i) + \"_\" + str(test_index) + \"_\" + str(support)\n",
    "                if use_alt_clustering_method:\n",
    "                    clusters_result = create_clusters_alternative(frequent_item_sets_list)\n",
    "                else:\n",
    "                    clusters_result = create_clusters(num_of_clusters[k], frequent_item_sets_list)\n",
    "                print(f\"{output_file} - clusters created\")\n",
    "                validate_and_score_clusters(clusters_result, all_concept_ids_counts, all_concept_ids, output_file, window_lengths[j], support, True)\n",
    "                test_index += 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "825d4a992f9f75b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def apply_fim(num_person, min_support, win_length, num_cluster, use_alt_sliding_window_method, use_alt_clustering_method, output_file):\n",
    "    event_mappings, all_concept_ids, all_concept_ids_counts, _ = create_datasets(person_nr=num_person, person_id=None)\n",
    "    return apply_fim_to_dataset(min_support, num_cluster, win_length, event_mappings, all_concept_ids, all_concept_ids_counts, output_file, use_alt_sliding_window_method, use_alt_clustering_method)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe1b7829fb2d7110"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
